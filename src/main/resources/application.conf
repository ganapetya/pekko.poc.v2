pekko {

  #this extension allows cross-node communication which considered as a best practice in Peko for cluster
  extensions = ["org.apache.pekko.cluster.pubsub.DistributedPubSub"]
  # Enhanced logging for debugging - set to DEBUG  - to see all the stuff
  loglevel = "DEBUG"
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
  
  # Additional logging configuration
  logger-startup-timeout = 30s
  # Enable specific loggers for sharding components
  logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"
  
  # Detailed sharding and cluster logging
  cluster {
    log-info = on
    log-info-verbose = on
    
    seed-nodes = [${?SEED_NODES}]
    roles = [${?NODE_ROLE}]
    name = ${?PEKKO_CLUSTER_NAME}

    pub-sub {
      # Actor name of the mediator actor, /system/distributedPubSubMediator
      name = distributedPubSubMediator
      
      # Start the mediator on members tagged with this role.
      # All members are used if undefined or empty.
      role = ""
      
      # The routing logic to use for 'Send'
      # Possible values: random, round-robin, broadcast
      routing-logic = random
      
      # How often the DistributedPubSubMediator should send out gossip information
      gossip-interval = 1s
      
      # Removed entries are pruned after this duration
      removed-time-to-live = 120s
      
      # Maximum number of elements to transfer in one message when synchronizing the registries.
      # Next chunk will be transferred in next round of gossip.
      max-delta-elements = 3000
      
      # The id of the dispatcher to use for DistributedPubSubMediator actors.
      # If not specified default dispatcher is used.
      # If specified you need to define the settings of the actual dispatcher.
      use-dispatcher = ""
    }

    failure-detector {
      threshold = 8.0           # Default is 10.0
      acceptable-heartbeat-pause = 3s  # Default is 6s
      heartbeat-interval = 1s   # Default is 1s
    }
    
    # Faster auto-down (use with caution in production)
    auto-down-unreachable-after = 10s
    
    # Cluster sharding configuration with enhanced logging
    sharding {
      handoff-timeout = 5s
      number-of-shards = 30
      passivate-idle-entity-after = 10 minutes
      log-level = "DEBUG"
      
      # Add these for more detailed shard allocation logging
      state-store-mode = "persistence"
      #remember-entities-store = "eventsourced"
      remember-entities = off
      verbose-debug-logging = on
      rebalance-interval = 5s 
      least-shard-allocation-strategy.rebalance-threshold = 1
      least-shard-allocation-strategy.max-simultaneous-rebalance = 5
      coordinator-singleton = {
        # Reduce the time coordinator waits for region termination
        termination-detection-interval = 1s  # Default is 3s
      }
      passivation {
        default-idle-strategy.idle-entity.timeout = 30s
      }
      # Apply dispatcher to specific entity types
      entity-recovery-strategy = "all"
      entity-recovery-constant-rate-strategy {
        frequency = 100ms
        number-of-entities = 5
      }
    }
    
    # Downing strategy
    downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"
    split-brain-resolver {
      active-strategy = keep-majority
      stable-after = 20s
    }
  }

  actor {
    provider = "cluster"

    # Enhanced debug settings
    debug {
      receive = on
      autoreceive = on
      lifecycle = on
      fsm = on
      unhandled = on
    }

    # Serialization configuration for cluster messages
    serializers {
      jackson-cbor = "org.apache.pekko.serialization.jackson.JacksonCborSerializer"
    }
    
    serialization-bindings {
      "com.example.cluster.serialize.CborSerializable" = jackson-cbor
      "org.apache.pekko.actor.typed.ActorRef" = jackson-cbor
    }
    
    allow-java-serialization = off


    #Custom dispatchers for some actors - case that we have arrange a special runtimes for actors
    dispatchers {
      case-companion-dispatcher {
        type = Dispatcher
        executor = "thread-pool-executor"
        thread-pool-executor {
          fixed-pool-size = 12
        }
        throughput = 1
      }

      log-analysis-dispatcher {
        type = Dispatcher
        executor = "thread-pool-executor"
        thread-pool-executor {
          fixed-pool-size = 24
        }
        throughput = 1
      }
    }
  }

  remote {
    artery {
      canonical.hostname = ${?CLUSTER_IP}
      canonical.port = ${?CLUSTER_PORT}
      
      # Add some debugging for remoting
      log-remote-lifecycle-events = on
    }
  }
  
  # Pekko Management - moved inside main pekko block
  management {
    cluster.bootstrap {
      contact-point-discovery {
        discovery-method = config
        service-name = ${?PEKKO_CLUSTER_NAME}
        port-name = "management"
        protocol = "tcp"
        
        # List of contact points
        config.contact-points = [${?SEED_NODES}]
      }
    }
    
    http {
      hostname = ${?CLUSTER_IP}
      port = ${?MANAGEMENT_PORT}
    }
  }

  # HTTP server configuration - moved inside main pekko block
  http {
    server {
      request-timeout = 60s
    }
  }

  # Persistence configuration
  persistence {
    journal {
      plugin = "jdbc-journal"
      // Enable the line below to automatically start the journal when the actorsystem is started
      // auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      plugin = "jdbc-snapshot-store"
      // Enable the line below to automatically start the snapshot-store when the actorsystem is started
      // auto-start-snapshot-stores = ["jdbc-snapshot-store"]
    }
    state {
      plugin = "jdbc-durable-state-store"
    }
  }
}

pekko-persistence-jdbc {
  shared-databases {
    slick {
      profile = "slick.jdbc.PostgresProfile$"
      db {
        
        host = "pekko-postgres"
        url = "jdbc:postgresql://pekko-postgres:5432/pekko?reWriteBatchedInserts=true"
        user = "pekko"
        password = "pekko"
        driver = "org.postgresql.Driver"
        numThreads = 5
        maxConnections = 5
        minConnections = 1
      }
    }
  }

}

# Application-specific HTTP server configuration (separate from pekko.http)
http {
  host = "0.0.0.0"
  port = ${?HTTP_PORT}
}

# HTTP routes configuration (application-specific)
http-routes {
  # Define available endpoints
  endpoints {
    # Health check endpoint
    health {
      path = "/health"
      method = "GET"
      type = "simple-response"
      response = "OK"
    }
    
    # Root health check
    root {
      path = "/"
      method = "GET"
      type = "simple-response"
      response = "Health check OK"
    }
    
    # Test endpoint for debugging
    test {
      path = "/test"
      method = "GET"
      type = "simple-response"
      response = "Test endpoint works!"
    }
    
    # Case resolution with JSON body
    api-resolve {
      path = "/api/v1/cases/resolve"
      method = "POST"
      type = "json-entity-handler"
    }
  }
}

# Kamon monitoring configuration (separate) - I did not concentrate on monitoring in this POC part
# but I provide Kamon to show we able to grab metrics
kamon {
  metric {
    tick-interval = 1 second
  }
  prometheus {
    start-embedded-server = yes
    embedded-server {
      hostname = "0.0.0.0"
      port = 9095
    }
  }
}

# JDBC configs for our persistence plugins

jdbc-journal {
  use-shared-db = "slick"
}

# the pekko-persistence-snapshot-store in use
jdbc-snapshot-store {
  use-shared-db = "slick"
}

# the pekko-persistence-query provider in use
jdbc-read-journal {
  use-shared-db = "slick"
}

# the pekko-persistence-jdbc provider in use for durable state store
jdbc-durable-state-store {
  use-shared-db = "slick"
}

# Kafka configuration for inter-cluster communication
kafka {
  bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}
  topics {
    deployment-requests = "deployment-requests"
    deployment-responses = "deployment-responses"
  }
  consumer-group = "pekko-cluster-group"
}